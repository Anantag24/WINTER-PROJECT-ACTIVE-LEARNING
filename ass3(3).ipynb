{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99693e01-fb0a-4b76-84f6-41ea24f07ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on initial labeled dataset (Custom CNN)...\n",
      "Epoch 1/10, Loss: 1.6084\n",
      "Epoch 2/10, Loss: 0.8402\n",
      "Epoch 3/10, Loss: 0.7468\n",
      "Epoch 4/10, Loss: 0.5912\n",
      "Epoch 5/10, Loss: 0.5325\n",
      "Epoch 6/10, Loss: 0.5559\n",
      "Epoch 7/10, Loss: 0.4494\n",
      "Epoch 8/10, Loss: 0.4247\n",
      "Epoch 9/10, Loss: 0.4324\n",
      "Epoch 10/10, Loss: 0.3919\n",
      "Initial Test Accuracy (Custom CNN): 80.16%\n",
      "\n",
      "Active Learning Iteration 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class PretrainedModel(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', num_classes=10):\n",
    "        super(PretrainedModel, self).__init__()\n",
    "        if model_name == 'resnet18':\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        elif model_name == 'vgg16':\n",
    "            self.model = models.vgg16(pretrained=True)\n",
    "            self.model.classifier[6] = nn.Linear(self.model.classifier[6].in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def prepare_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def split_dataset(dataset, initial_labeled_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    labeled_indices = np.random.choice(indices, size=initial_labeled_size, replace=False)\n",
    "    unlabeled_indices = [i for i in indices if i not in labeled_indices]\n",
    "    return Subset(dataset, labeled_indices), Subset(dataset, unlabeled_indices)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(data_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def get_uncertain_samples(model, data_loader, num_samples, strategy=\"entropy\"):\n",
    "    model.eval()\n",
    "    uncertainties = []\n",
    "    with torch.no_grad():\n",
    "        for images, indices in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            if strategy == \"entropy\":\n",
    "                entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
    "                uncertainties.extend(zip(entropy.tolist(), indices.tolist()))\n",
    "            elif strategy == \"least_confidence\":\n",
    "                confidence, _ = torch.max(probs, dim=1)\n",
    "                uncertainties.extend(zip(-confidence.tolist(), indices.tolist()))\n",
    "            elif strategy == \"margin\":\n",
    "                sorted_probs, _ = probs.sort(dim=1, descending=True)\n",
    "                margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n",
    "                uncertainties.extend(zip(-margin.tolist(), indices.tolist()))\n",
    "    \n",
    "    uncertainties.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [index for _, index in uncertainties[:num_samples]]\n",
    "\n",
    "def get_diverse_samples(model, data_loader, num_samples, diversity_metric=\"cosine_similarity\"):\n",
    "    model.eval()\n",
    "    features_list, indices = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, img_indices in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            features = F.relu(model.conv1(images))\n",
    "            features = F.relu(model.conv2(features)).view(-1, 64 * 7 * 7)\n",
    "            features_list.append(features)\n",
    "            indices.extend(img_indices)\n",
    "\n",
    "    features = torch.cat(features_list, dim=0)\n",
    "\n",
    "    if diversity_metric == \"cosine_similarity\":\n",
    "        similarity_matrix = F.cosine_similarity(features.unsqueeze(1), features.unsqueeze(0), dim=2)\n",
    "        uncertainties = 1 - similarity_matrix.max(dim=1)[0]\n",
    "    elif diversity_metric == \"l2_norm\":\n",
    "        center = features.mean(dim=0)\n",
    "        distances = torch.norm(features - center, dim=1)\n",
    "        uncertainties = distances\n",
    "    elif diversity_metric == \"kl_divergence\":\n",
    "        probs = F.softmax(model(features), dim=1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
    "        uncertainties = entropy\n",
    "\n",
    "    uncertainties, indices = zip(*sorted(zip(uncertainties, indices), reverse=True, key=lambda x: x[0]))\n",
    "    return list(indices[:num_samples])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data preparation\n",
    "    train_dataset, test_dataset = prepare_data()\n",
    "    initial_labeled_size = 1000\n",
    "    labeled_set, unlabeled_set = split_dataset(train_dataset, initial_labeled_size)\n",
    "\n",
    "    # Data loaders\n",
    "    labeled_loader = DataLoader(labeled_set, batch_size=64, shuffle=True)\n",
    "    unlabeled_loader = DataLoader(unlabeled_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Initialize custom CNN model\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train initial model on labeled set\n",
    "    print(\"Training on initial labeled dataset (Custom CNN)...\")\n",
    "    train_model(model, labeled_loader, optimizer, criterion, epochs=10)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    print(f\"Initial Test Accuracy (Custom CNN): {accuracy:.2f}%\")\n",
    "\n",
    "    # Active learning iterations\n",
    "    num_iterations = 5\n",
    "    num_samples = 100\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nActive Learning Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Select uncertain samples\n",
    "        new_indices_uncertainty = get_uncertain_samples(model, unlabeled_loader, num_samples, strategy=\"entropy\")\n",
    "        \n",
    "        # Select diverse samples\n",
    "        new_indices_diversity = get_diverse_samples(model, unlabeled_loader, num_samples, diversity_metric=\"cosine_similarity\")\n",
    "\n",
    "        # Combine indices and remove duplicates\n",
    "        new_indices = list(set(new_indices_uncertainty + new_indices_diversity))\n",
    "\n",
    "        # Convert labeled_set.indices to a list and update subsets\n",
    "        labeled_set_indices = list(labeled_set.indices)\n",
    "        labeled_set_indices.extend(new_indices)\n",
    "        unlabeled_set_indices = [i for i in unlabeled_set.indices if i not in new_indices]\n",
    "\n",
    "        # Update labeled and unlabeled subsets\n",
    "        labeled_set = Subset(train_dataset, labeled_set_indices)\n",
    "        unlabeled_set = Subset(train_dataset, unlabeled_set_indices)\n",
    "\n",
    "        # Update data loaders\n",
    "        labeled_loader = DataLoader(labeled_set, batch_size=64, shuffle=True)\n",
    "        unlabeled_loader = DataLoader(unlabeled_set, batch_size=64, shuffle=False)\n",
    "\n",
    "        # Fine-tune pretrained model\n",
    "        pretrained_model = PretrainedModel().to(device)\n",
    "        pretrained_optimizer = optim.Adam(pretrained_model.parameters(), lr=0.001)\n",
    "        pretrained_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        print(\"Fine-tuning Pretrained Model (ResNet18)...\")\n",
    "        train_model(pretrained_model, labeled_loader, pretrained_optimizer, pretrained_criterion, epochs=5)\n",
    "\n",
    "        # Evaluate pretrained model on test set\n",
    "        accuracy = evaluate_model(pretrained_model, test_loader)\n",
    "        print(f\"Iteration {iteration + 1} Test Accuracy (Pretrained Model): {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a8f03-76df-4f8a-8391-767e3f52c95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
