{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5b9d4f3-52ee-450b-b6bf-ee9f4b51ac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on initial labeled dataset...\n",
      "Epoch 1/10, Loss: 0.9059\n",
      "Epoch 2/10, Loss: 0.5100\n",
      "Epoch 3/10, Loss: 0.4330\n",
      "Epoch 4/10, Loss: 0.3943\n",
      "Epoch 5/10, Loss: 0.3459\n",
      "Epoch 6/10, Loss: 0.3124\n",
      "Epoch 7/10, Loss: 0.2916\n",
      "Epoch 8/10, Loss: 0.2711\n",
      "Epoch 9/10, Loss: 0.2371\n",
      "Epoch 10/10, Loss: 0.1944\n",
      "Initial Test Accuracy: 86.06%\n",
      "\n",
      "Active Learning Iteration 1\n",
      "Epoch 1/5, Loss: 0.1763\n",
      "Epoch 2/5, Loss: 0.1627\n",
      "Epoch 3/5, Loss: 0.1394\n",
      "Epoch 4/5, Loss: 0.1268\n",
      "Epoch 5/5, Loss: 0.1108\n",
      "Iteration 1 Test Accuracy: 86.18%\n",
      "\n",
      "Active Learning Iteration 2\n",
      "Epoch 1/5, Loss: 0.0901\n",
      "Epoch 2/5, Loss: 0.0771\n",
      "Epoch 3/5, Loss: 0.0797\n",
      "Epoch 4/5, Loss: 0.0487\n",
      "Epoch 5/5, Loss: 0.0626\n",
      "Iteration 2 Test Accuracy: 87.49%\n",
      "\n",
      "Active Learning Iteration 3\n",
      "Epoch 1/5, Loss: 0.0313\n",
      "Epoch 2/5, Loss: 0.0436\n",
      "Epoch 3/5, Loss: 0.0287\n",
      "Epoch 4/5, Loss: 0.0254\n",
      "Epoch 5/5, Loss: 0.0125\n",
      "Iteration 3 Test Accuracy: 87.31%\n",
      "\n",
      "Active Learning Iteration 4\n",
      "Epoch 1/5, Loss: 0.0122\n",
      "Epoch 2/5, Loss: 0.0153\n",
      "Epoch 3/5, Loss: 0.0082\n",
      "Epoch 4/5, Loss: 0.0073\n",
      "Epoch 5/5, Loss: 0.0033\n",
      "Iteration 4 Test Accuracy: 87.74%\n",
      "\n",
      "Active Learning Iteration 5\n",
      "Epoch 1/5, Loss: 0.0023\n",
      "Epoch 2/5, Loss: 0.0018\n",
      "Epoch 3/5, Loss: 0.0015\n",
      "Epoch 4/5, Loss: 0.0013\n",
      "Epoch 5/5, Loss: 0.0012\n",
      "Iteration 5 Test Accuracy: 87.79%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 classes for Fashion-MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def prepare_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, initial_labeled_size):\n",
    "    indices = list(range(len(dataset)))\n",
    "    labeled_indices = np.random.choice(indices, size=initial_labeled_size, replace=False)\n",
    "    unlabeled_indices = [i for i in indices if i not in labeled_indices]\n",
    "    return Subset(dataset, labeled_indices), Subset(dataset, unlabeled_indices)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(data_loader):.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    train_dataset, test_dataset = prepare_data()\n",
    "    initial_labeled_size = 5000  # Choose an appropriate initial labeled size\n",
    "    labeled_set, unlabeled_set = split_dataset(train_dataset, initial_labeled_size)\n",
    "\n",
    "    labeled_loader = DataLoader(labeled_set, batch_size=64, shuffle=True)\n",
    "    unlabeled_loader = DataLoader(unlabeled_set, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"Training on initial labeled dataset...\")\n",
    "    train_model(model, labeled_loader, optimizer, criterion, epochs=10)\n",
    "\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    print(f\"Initial Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    num_iterations = 5\n",
    "    num_samples = 1000\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nActive Learning Iteration {iteration + 1}\")\n",
    "        \n",
    "        train_model(model, labeled_loader, optimizer, criterion, epochs=5)\n",
    "        \n",
    "        accuracy = evaluate_model(model, test_loader)\n",
    "        print(f\"Iteration {iteration + 1} Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2bf31-cc04-4989-9c68-ef900fbcbd73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
