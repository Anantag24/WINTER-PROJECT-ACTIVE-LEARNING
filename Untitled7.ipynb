{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006960ff-7e5b-436f-ab4e-d6b5aeedd551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./dataset\\MNIST\\raw\n",
      "\n",
      "Epoch 1/15, Average Loss: 0.3738\n",
      "Epoch 2/15, Average Loss: 0.1055\n",
      "Epoch 3/15, Average Loss: 0.0754\n",
      "Epoch 4/15, Average Loss: 0.0605\n",
      "Epoch 5/15, Average Loss: 0.0514\n",
      "Epoch 6/15, Average Loss: 0.0450\n",
      "Epoch 7/15, Average Loss: 0.0384\n",
      "Epoch 8/15, Average Loss: 0.0331\n",
      "Epoch 9/15, Average Loss: 0.0295\n",
      "Epoch 10/15, Average Loss: 0.0273\n",
      "Epoch 11/15, Average Loss: 0.0243\n",
      "Epoch 12/15, Average Loss: 0.0214\n",
      "Epoch 13/15, Average Loss: 0.0201\n",
      "Epoch 14/15, Average Loss: 0.0184\n",
      "Epoch 15/15, Average Loss: 0.0165\n",
      "Test Accuracy: 99.10%\n",
      "Average Confidence Score: 0.0061\n",
      "Average Prediction Entropy: 0.0184\n",
      "Average Margin Sampling: 0.0114\n",
      "Average Cosine Similarity: 0.4843\n",
      "Average L2 Distance: 0.9930\n",
      "Average KL Divergence: 1.6027\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Define the LeNet-5 architecture with feature extraction\n",
    "class LeNet5FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5FeatureExtractor, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(1, 6, kernel_size=5)  # Convolutional Layer 1\n",
    "        self.pooling_layer1 = nn.AvgPool2d(kernel_size=2, stride=2)  # Pooling Layer 1\n",
    "        self.conv_layer2 = nn.Conv2d(6, 16, kernel_size=5)  # Convolutional Layer 2\n",
    "        self.pooling_layer2 = nn.AvgPool2d(kernel_size=2, stride=2)  # Pooling Layer 2\n",
    "        self.fc_layer1 = nn.Linear(16 * 4 * 4, 120)  # Fully Connected Layer 1\n",
    "        self.fc_layer2 = nn.Linear(120, 84)  # Fully Connected Layer 2\n",
    "        self.output_layer = nn.Linear(84, num_classes)  # Output Layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pooling_layer1(F.relu(self.conv_layer1(x)))  # Conv1 -> ReLU -> Pool1\n",
    "        x = self.pooling_layer2(F.relu(self.conv_layer2(x)))  # Conv2 -> ReLU -> Pool2\n",
    "        extracted_features = x.view(-1, 16 * 4 * 4)  # Flatten\n",
    "        x = F.relu(self.fc_layer1(extracted_features))  # FC1 -> ReLU\n",
    "        x = F.relu(self.fc_layer2(x))  # FC2 -> ReLU\n",
    "        logits = self.output_layer(x)  # Output\n",
    "        return logits, extracted_features  # Return logits and features\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64  # Increased batch size\n",
    "learning_rate = 0.0005  # Adjusted learning rate\n",
    "epochs = 15  # Increased number of epochs\n",
    "\n",
    "# Data preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize grayscale images\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_data = datasets.MNIST(root='./dataset', train=True, download=True, transform=preprocess)\n",
    "test_data = datasets.MNIST(root='./dataset', train=False, download=True, transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = LeNet5FeatureExtractor().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training phase\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions, _ = model(inputs)\n",
    "        loss = loss_function(predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Define uncertainty metrics\n",
    "def compute_uncertainty_metrics(logits):\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    # Least Confidence\n",
    "    confidence_scores = 1 - probabilities.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "    # Prediction Entropy\n",
    "    probabilities[probabilities == 0] = 1e-10  # Avoid log(0)\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities), dim=1).cpu().numpy()\n",
    "\n",
    "    # Margin Sampling\n",
    "    sorted_probs, _ = torch.sort(probabilities, descending=True)\n",
    "    margins = 1 - (sorted_probs[:, 0] - sorted_probs[:, 1]).cpu().numpy()\n",
    "\n",
    "    return confidence_scores, entropy, margins\n",
    "\n",
    "# Define diversity metrics\n",
    "def compute_diversity_metrics(feature_vectors, top_k=5):\n",
    "    cosine_distances = pairwise_distances(feature_vectors.cpu().numpy(), metric='cosine')\n",
    "    cosine_similarity = 1 - np.mean(cosine_distances[:, 1:top_k + 1], axis=1)\n",
    "\n",
    "    l2_distances = pairwise_distances(feature_vectors.cpu().numpy(), metric='euclidean')\n",
    "    avg_l2_distances = np.mean(l2_distances[:, 1:top_k + 1], axis=1)\n",
    "\n",
    "    return cosine_similarity, avg_l2_distances\n",
    "\n",
    "# Define KL divergence\n",
    "def compute_kl_divergence(logits, pairwise_similarities, top_k=5):\n",
    "    epsilon = 1e-10\n",
    "    kl_scores = []\n",
    "\n",
    "    for i in range(len(logits)):\n",
    "        current_prob = F.softmax(logits[i], dim=0)\n",
    "        current_log_prob = torch.log(current_prob + epsilon)\n",
    "\n",
    "        neighbor_indices = pairwise_similarities[i, 1:top_k + 1].astype(int)\n",
    "        neighbor_probs = torch.mean(F.softmax(logits[neighbor_indices], dim=1), dim=0) + epsilon\n",
    "\n",
    "        kl_score = F.kl_div(current_log_prob, neighbor_probs, reduction='batchmean').item()\n",
    "        kl_scores.append(kl_score)\n",
    "\n",
    "    return kl_scores\n",
    "\n",
    "# Evaluation phase\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "num_samples = 0\n",
    "\n",
    "# Metrics storage\n",
    "confidence_scores_list = []\n",
    "entropy_list = []\n",
    "margins_list = []\n",
    "cosine_similarity_list = []\n",
    "l2_distances_list = []\n",
    "kl_divergence_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, features = model(inputs)\n",
    "\n",
    "        _, predictions = torch.max(logits, dim=1)\n",
    "        total_correct += (predictions == targets).sum().item()\n",
    "        num_samples += targets.size(0)\n",
    "\n",
    "        # Compute metrics\n",
    "        conf_scores, entropy, margins = compute_uncertainty_metrics(logits)\n",
    "        confidence_scores_list.extend(conf_scores)\n",
    "        entropy_list.extend(entropy)\n",
    "        margins_list.extend(margins)\n",
    "\n",
    "        features_normalized = F.normalize(features, p=2, dim=1)\n",
    "        cosine_sim, l2_dist = compute_diversity_metrics(features_normalized)\n",
    "        cosine_similarity_list.extend(cosine_sim)\n",
    "        l2_distances_list.extend(l2_dist)\n",
    "\n",
    "        feature_dists = pairwise_distances(features.cpu().numpy(), metric='cosine')\n",
    "        kl_scores = compute_kl_divergence(logits, feature_dists)\n",
    "        kl_divergence_list.extend(kl_scores)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = total_correct / num_samples * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Average Confidence Score: {np.mean(confidence_scores_list):.4f}\")\n",
    "print(f\"Average Prediction Entropy: {np.mean(entropy_list):.4f}\")\n",
    "print(f\"Average Margin Sampling: {np.mean(margins_list):.4f}\")\n",
    "print(f\"Average Cosine Similarity: {np.mean(cosine_similarity_list):.4f}\")\n",
    "print(f\"Average L2 Distance: {np.mean(l2_distances_list):.4f}\")\n",
    "print(f\"Average KL Divergence: {np.mean(kl_divergence_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142b96b-f468-4e38-abcf-a640d9d9ed37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
